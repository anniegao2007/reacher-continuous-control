{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reacher Continuous Control\n",
    "\n",
    "## The Problem\n",
    "We are training reinforcement learning agents with double-jointed arms to follow moving targets in the Unity ML-Agents Reacher environment. A reward of +0.1 is given for each time step that an agent's hand is in the target position. We want to maximize the number of time steps that the agent maintains its position in the target position.\n",
    "\n",
    "### The Environment\n",
    "We are using the Reacher environment from the Unity ML-Agents plugin.\n",
    "\n",
    "For this task, each state consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers in the range \\[-1, 1\\], corresponding to torque applicable to two joints. We simultaneously train 20 identical agents, each with their own copy of the environment. At each episode, we store the average of the scores accumulated by all the agents in a buffer of size 100. The task is considered solved when the average score of the buffer reaches or exceeds 30.\n",
    "\n",
    "### Set Up\n",
    "Make sure [Unity Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) is installed. Then run the following line to install all other required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the Unity environment, change the ```file_name``` parameter based on your operating system, as specified below:\n",
    " * Windows (x86): \"envs/Reacher_Windows_x86/Reacher.exe\"\n",
    " * Windows (x86_64): \"envs/Reacher_Windows_x86_64/Reacher.exe\"\n",
    " * Linux (x86): \"envs/Reacher_Linux/Reacher.x86\"\n",
    " * Linux (x86_64): \"envs/Reacher_Linux/Reacher.x86_64\"\n",
    " * Mac: \"envs/Reacher.app\"\n",
    "\n",
    "For example,\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"envs/Reacher_Linux/Reacher.x86_64\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"envs/Reacher_Linux/Reacher.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine the environment in more detail, run the following code chunk. As can be seen, there are 20 agents, each state consists of 37 components, and each action consists of 4 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to build the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Solution\n",
    "\n",
    "### Deep Deterministic Policy Gradient (DDPG)\n",
    "We implement the architecture presented in [this paper (Lillicrap et al, 2015)](https://arxiv.org/pdf/1509.02971.pdf) on Deep Deterministic Policy Gradients, which uses an Actor-Critic dual-network structure to combine both policy gradient methods (the actor) and action-value approximations (the critic) into a model-free framework that achieves satisfactory performance in continuous action spaces. The structure of both networks is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fan_in(size):  # helper method for initializing weights\n",
    "    f = size[0]\n",
    "    bound = 1.0 / np.sqrt(f)\n",
    "    return torch.Tensor(size).uniform_(-bound, bound)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden1=256, hidden2=128, final_weights_init=3e-3):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.final_weights_init = final_weights_init\n",
    "        self.fc1 = nn.Linear(self.state_size, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, self.action_size)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.fc1.weight.data = fan_in(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fan_in(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-self.final_weights_init, self.final_weights_init)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden1=256, hidden2=128, final_weights_init=3e-3):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.final_weights_init = final_weights_init\n",
    "        self.fc1 = nn.Linear(self.state_size, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1+self.action_size, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, self.action_size)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.fc1.weight.data = fan_in(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fan_in(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-self.final_weights_init, self.final_weights_init)\n",
    "        \n",
    "    def forward(self, state_action):\n",
    "        state, action = state_action\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(torch.cat([x, action], 1)))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay\n",
    "Like Deep Q-Networks (DQN), the DDPG agent also makes use of experience replay, where interactions with the environment are stored in _(state, action, reward, next_state, done)_ tuples in a buffer of capacity 1e5 and sampled at random in decorrelated minibatches of size 128 to use in updating all the networks. The structure of the replay buffer is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size, minibatch_size, device):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.device = device\n",
    "        self.experience = namedtuple(\"Experience\", field_names=['state', 'action', 'reward', 'next_state', 'done'])\n",
    "        self.size = 0\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append(self.experience(state, action, reward, next_state, done))\n",
    "        self.size += 1\n",
    "        \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.buffer, k=self.minibatch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([1 if e.done else 0 for e in experiences if e is not None])).float().to(self.device)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "        \n",
    "    def get_len(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Noise for Action Exploration\n",
    "\n",
    "The DDPG paper makes use of the Ornstein-Uhlenbeck Process for generating random noise for momentum-based processes. This noise will encourage the agent to explore new actions, and the magnitude of the noise decreases over time as the variance decays with each episode, down to a minimum value of 0.05, where it remains for the remainder of training. For this task, we use parameters of mu=0, theta=0.15, sigma=0.2, and sigma_decay=0.98."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeck():\n",
    "    def __init__(self, num_agents, action_size, mu=0, theta=0.15, sigma=0.2, sigma_min=0.05, sigma_decay=0.98):\n",
    "        self.num_agents = num_agents\n",
    "        self.action_size = action_size\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.sigma_min = sigma_min\n",
    "        self.sigma_decay = sigma_decay\n",
    "        self.theta = theta\n",
    "        self.prev_val = np.zeros((self.num_agents, self.action_size))\n",
    "    \n",
    "    def sample(self):\n",
    "        val = self.prev_val + self.theta * (self.mu - self.prev_val) * self.sigma * np.random.normal(size=(self.num_agents, self.action_size))\n",
    "        self.prev_val = val\n",
    "        return val\n",
    "    \n",
    "    def reset(self):\n",
    "        self.prev_val = np.zeros((self.num_agents, self.action_size))\n",
    "        self.sigma = max(self.sigma_min, self.sigma*self.sigma_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent\n",
    "\n",
    "The RL agent is shown below. It has two copies of each of the actor and the critic networks; while all the target and local networks are updated at every time step, the target networks are updated much more slowly using the *soft\\_update* function, parametrized by tau=0.001. This reduces unwanted sequential dependencies in the data that could cause the networks to update their parameters based on moving targets, and contributes to more stability during training. The actor is updated with a policy loss, and the critic is updated using a Mean-Squared Error loss for the TD error (with gamma=0.99). Although the code allows for updating the networks every *update\\_freq* steps, using *update\\_freq*=1 appears to have the best results.\n",
    "\n",
    "We use Adam optimizers with learning rates of 1e-4 to train both the actor and the critic networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_Agent():\n",
    "    def __init__(self, num_agents, state_size, action_size, buffer_capacity=1e5, minibatch_size=128, update_freq=1, tau=1e-3, gamma=0.99, lr_actor=1e-4, lr_critic=1e-4):\n",
    "        self.num_agents = num_agents\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.buffer = ReplayBuffer(int(buffer_capacity), minibatch_size, self.device)\n",
    "        self.noise = OrnsteinUhlenbeck(num_agents=num_agents, action_size=action_size)\n",
    "        self.num_steps = 0\n",
    "        \n",
    "        self.actor_local = Actor(state_size, action_size).to(self.device)\n",
    "        self.actor_target = Actor(state_size, action_size).to(self.device)\n",
    "        self.copy_weights(self.actor_target, self.actor_local)\n",
    "        self.optim_actor = optim.Adam(self.actor_local.parameters(), lr=lr_actor)\n",
    "        self.critic_local = Critic(state_size, action_size).to(self.device)\n",
    "        self.critic_target = Critic(state_size, action_size).to(self.device)\n",
    "        self.copy_weights(self.critic_target, self.critic_local)\n",
    "        self.optim_critic = optim.Adam(self.critic_local.parameters(), lr=lr_critic) #, weight_decay=1e-2)\n",
    "        \n",
    "        self.update_freq = update_freq\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def copy_weights(self, target_network, source_network):\n",
    "        for target_param, param_to_copy in zip(target_network.parameters(), source_network.parameters()):\n",
    "            target_param.data.copy_(param_to_copy.data)\n",
    "    \n",
    "    def select_action(self, states):  # states is shape (num_agents, state_size)\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_local(states)  # shape (num_agents, action_size)\n",
    "        self.actor_local.train()\n",
    "        return np.clip(actions.cpu().data.numpy() + self.noise.sample(), -1, 1)\n",
    "    \n",
    "    def update(self, states, actions, rewards, next_states, dones):\n",
    "        for i in range(self.num_agents):\n",
    "            self.buffer.add(states[i], actions[i], rewards[i], next_states[i], dones[i])\n",
    "        self.num_steps = (self.num_steps + 1) % self.update_freq\n",
    "        if self.num_steps == 0 and self.buffer.get_len() >= self.minibatch_size:\n",
    "            self.update_local()\n",
    "            self.soft_update(self.actor_target, self.actor_local)\n",
    "            self.soft_update(self.critic_target, self.critic_local)\n",
    "    \n",
    "    def update_local(self):\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample()\n",
    "        \n",
    "        # update critic\n",
    "        next_actions_target = self.actor_target(next_states)\n",
    "        future_rewards_target = self.critic_target((next_states, next_actions_target))\n",
    "        value_target = rewards + self.gamma*future_rewards_target*(1-dones)\n",
    "        value_local = self.critic_local((states, actions))\n",
    "        critic_loss = F.mse_loss(value_local, value_target)\n",
    "        self.optim_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.optim_critic.step()\n",
    "        \n",
    "        # update actor\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local((states, actions_pred)).mean()\n",
    "        self.optim_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optim_actor.step()\n",
    "    \n",
    "    def soft_update(self, target_network, local_network):\n",
    "        for target_param, local_param in zip(target_network.parameters(), local_network.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1-self.tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it Together\n",
    "\n",
    "Now we are ready to train the agents. We use a cap of 500 episodes, and allow each episode to play out from start to finish. At the end of each episode, the average score from the 20 agents in that episode is stored into the buffer *score\\_window*, and the task is completed when the average of the values within *score\\_window* reaches or exceeds +30. The actor and critic models are then saved into _actor.pth_ and _critic.pth_, respectively, and the learning curve is plotted to show the average score across the agents at every training episode. This figure is saved into *learning\\_curve.png*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "num_agents, state_size = states.shape\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "num_episodes = 500\n",
    "\n",
    "agent = DDPG_Agent(num_agents=num_agents, state_size=state_size, action_size=action_size)\n",
    "score_window = deque(maxlen=100)\n",
    "score_record = []\n",
    "highest_avg_score = 0\n",
    "\n",
    "print(f'Training starts with {num_agents} agents!')\n",
    "for ep in range(1, num_episodes+1):\n",
    "    scores = np.zeros(num_agents)\n",
    "    agent.noise.reset()\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    while True:\n",
    "        actions = agent.select_action(states)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        scores += rewards\n",
    "        dones = env_info.local_done\n",
    "        agent.update(states, actions, rewards, next_states, dones)\n",
    "        states = next_states\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    avg_score = np.mean(scores)\n",
    "    score_window.append(avg_score)\n",
    "    score_record.append(avg_score)\n",
    "    if avg_score > highest_avg_score:\n",
    "        highest_avg_score = avg_score\n",
    "    print(f'\\rEpisode {ep}/{num_episodes}: Average Score = {avg_score}, Highest Average Score = {highest_avg_score}', end=\"\")\n",
    "    if np.mean(score_window) >= 30:\n",
    "        print(f'\\nEnvironment solved in {ep} episodes!   Average Score: {np.mean(score_window)}')\n",
    "        break\n",
    "        \n",
    "# save the trained model\n",
    "torch.save(agent.actor_local.state_dict(), 'actor.pth')\n",
    "torch.save(agent.critic_local.state_dict(), 'critic.pth')\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(score_record)), score_record)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.savefig('learning_curve.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the runs solved this task in 164 episodes, and the models for that run are saved in _actor.pth_ and _critic.pth_. The learning curve is pictured below:\n",
    "\n",
    "![learning curve](learning_curve.png)\n",
    "\n",
    "As can be seen, the curve rises steadily upward, and the agents are able to achieve an average score of +30 in a single episode by around episode 100. The scores continue to grow until about episode 110, where the average score per episode stabilizes to between about 33 to 38. The task is solved by episode 164."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the task is solved, we can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future work involves more fine-grained hyperparameter tuning and a comparison of the performance of other reinforcement learning methods like A3C, A2C, PPO, and D4PG. We can also explore the generalization capacity of this DDPG architecture with its current set of hyperparameters, and test it on a variety of other tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
